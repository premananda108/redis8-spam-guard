# Открывая "черный ящик": Путешествие в мир векторного поиска и классификации спама

*Как я создал систему борьбы со спамом на Redis 8 Vector Sets и что узнал о внутренней кухне машинного обучения*

---

## Зачем заглядывать под капот?

В эпоху ChatGPT и готовых AI API легко забыть, что за красивыми интерфейсами скрывается сложная математика. Когда я начинал этот проект, у меня была простая цель: **понять, как на самом деле работает векторный поиск**, а не просто использовать его как "магическую" функцию.

Результатом стала система классификации спама для dev.to, которая позволяет проследить весь путь от сырого текста до финального решения "спам/не спам". Каждый шаг прозрачен, измерим и объясним.

## Анатомия "обучения": что происходит на самом деле?

### Первое открытие: мы не обучаем модель

Когда говорят "обучение модели", обычно представляют нейросеть, которая учится на данных. В моем случае все оказалось иначе. **Я не обучаю модель** — я использую уже готовую `all-MiniLM-L6-v2` как "переводчик" с человеческого языка на язык математики.

```python
# Это не обучение модели, а создание базы знаний
model = SentenceTransformer('all-MiniLM-L6-v2')
text_vector = model.encode("How to earn $1000 in a week!")
# Получаем массив из 384 чисел, отражающих смысл текста
```

Настоящее "обучение" происходит на уровне создания базы данных векторов с правильными метками.

### Второе открытие: автоматическая разметка — это искусство эвристик

Самая большая проблема любого ML проекта — получение размеченных данных. У меня не было тысяч постов с метками "спам/не спам", поэтому пришлось создать `SpamLabelGenerator` — систему правил для автоматической разметки:

```python
def calculate_spam_score(self, article: Dict) -> float:
    score = 0.0
    
    # Спам-слова в заголовке (высокий вес)
    for keyword in self.spam_keywords:
        if keyword in title:
            score += 0.3
    
    # Короткие посты с низким вовлечением
    if reading_time < 2 and reactions < 5:
        score += 0.3
    
    # Слишком много тегов
    if len(tags) > 10:
        score += 0.2
    
    return min(max(score, 0.0), 1.0)
```

**Трудность**: Как найти баланс между точностью и полнотой? Слишком строгие правила — пропускаем спам, слишком мягкие — блокируем легитимный контент.

**Решение**: Добавил элемент случайности и множественные критерии. Система анализирует не только текст, но и метрики вовлеченности, профиль автора, время публикации.

### Третье открытие: векторы — это не просто числа

Изначально я думал, что вектор — это просто "отпечаток" текста. На практике оказалось, что можно создавать гибридные векторы, комбинируя семантические эмбеддинги с числовыми признаками:

```python
# Текстовый вектор (384 измерения)
text_vector = self.model.encode(combined_text)

# Числовые признаки (3 измерения)
numeric_features = np.array([
    reading_time,
    user_followers,
    len(tags)
])

# Объединяем в единый вектор (387 измерений)
final_vector = np.concatenate([text_vector, numeric_features])
```

**Инсайт**: Векторное пространство можно "обогащать" любыми числовыми данными. Посты становятся похожими не только по смыслу, но и по структуре.

## Redis как "картотека смыслов"

### Четвертое открытие: Vector Sets — это не просто поиск

Redis Vector Sets оказался не просто базой данных, а полноценным движком для семантического поиска. Создание индекса:

```python
schema = (
    "vector", "VECTOR", "HNSW", "6", 
    "TYPE", "FLOAT32", "DIM", str(self.vector_dim), 
    "DISTANCE_METRIC", "L2",
    "label", "TAG",
    "title", "TEXT", 
    "url", "TAG"
)
```

**HNSW (Hierarchical Navigable Small World)** — это не просто алгоритм, это целая философия поиска. Он строит многоуровневый граф связей между векторами, позволяя находить ближайших соседей за логарифмическое время.

### Пятое открытие: k-NN — это демократия векторов

Классификация новых постов происходит через "голосование" ближайших соседей:

```python
# Находим 9 ближайших соседей
similar_posts = await self.find_similar_posts(query_vector, k=9)

# Собираем их метки
labels = [post['label'] for post in similar_posts]

# Побеждает большинство
predicted_label = Counter(labels).most_common(1)[0][0]
confidence = vote_count / total_neighbors
```

**Магия в деталях**: Почему именно k=9? После экспериментов выяснилось, что нечетное число соседей избегает ничьих, а 9 дает достаточную выборку без потери точности.

## Трудности и их решения

### Проблема 1: Качество автоматической разметки

**Трудность**: Как проверить, что автоматические метки корректны?

**Решение**: Добавил детальное логирование и метрики качества:

```python
metrics = {
    'accuracy': (tp + tn) / total,
    'precision': tp / (tp + fp),
    'recall': tp / (tp + fn),
    'f1_score': 2 * (precision * recall) / (precision + recall)
}
```

### Проблема 2: Дисбаланс данных — слишком мало спама

**Трудность**: Данные с dev.to API содержали очень мало реального спама. Модель не могла эффективно обучаться — ей не хватало примеров "плохого" поведения.

**Решение**: Создали синтетический датасет спама и подмешали его к реальным данным:

```python
# Загружаем синтетические спам-посты
try:
    with open('spam_dataset.json', 'r', encoding='utf-8') as f:
        spam_articles = json.load(f)
        # Помечаем как известный спам
        for article in spam_articles:
            article['is_known_spam'] = True
        articles.extend(spam_articles)
except FileNotFoundError:
    logger.warning("spam_dataset.json not found")
```

**Состав синтетического датасета** (50 примеров):
- Схемы быстрого заработка и крипто-мошенничество
- Фишинговые ссылки и поддельные уведомления безопасности  
- Продажа SEO-услуг и накрутка подписчиков
- Сомнительные курсы и "чудо-товары"

**Результат**: Получили сбалансированную выборку, модель достигла precision = 1.0 (ноль ложных срабатываний).

## Что я узнал о "черном ящике"

### 1. Векторы имеют геометрию

Похожие по смыслу тексты действительно располагаются близко в векторном пространстве. Это не метафора — это математическая реальность, которую можно измерить через косинусное расстояние или евклидову норму.

### 2. Качество данных важнее алгоритма

Самый продвинутый алгоритм не поможет, если данные плохо размечены. 80% успеха проекта — это качественная подготовка обучающей выборки.

### 3. Объяснимость можно встроить

Каждое решение системы сопровождается детальным объяснением:

```python
reasoning = [
    "Similar to known spam posts (via Redis)",
    "Contains spam keywords: 'earn money'",
    "Low follower count (5)",
    "Short reading time (1 minute)"
]
```

### 4. Redis как основа архитектуры

Система полностью построена вокруг возможностей Redis Vector Sets. Без Redis нет системы — это не просто база данных, а ядро всей логики классификации.

## Практические результаты

После обучения на ~2000 постов система показывает:
- **Точность**: 78-85%
- **Скорость**: ~50ms на классификацию
- **Объяснимость**: 100% решений с детальным обоснованием

## Выводы: зачем открывать "черный ящик"?

Этот проект научил меня, что Redis Vector Sets — это не просто новая фича, а революция в подходе к семантическому поиску:

1. **Скорость**: Векторный поиск работает за миллисекунды даже на тысячах записей
2. **Простота**: Не нужны сложные ML фреймворки — Redis берет на себя всю тяжелую работу
3. **Масштабируемость**: Система готова к росту без архитектурных изменений
4. **Прозрачность**: Каждое решение можно проследить и объяснить

Redis Vector Sets превратил сложную задачу семантического поиска в элегантное и понятное решение.

---

*Полный код проекта доступен на GitHub. Система работает в режиме реального времени и готова к демонстрации.*